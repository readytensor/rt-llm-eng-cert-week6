# Makefile: Build llama.cpp, convert HF model to FP16 GGUF, quantize, and (optionally) shard/upload
# Works on Windows (quoted paths) and Unix-like systems. Assumes you already downloaded the HF model.

# ---- User-configurable variables ----
# Path to your downloaded Hugging Face model directory (change this to your local model path)
MODEL_PATH ?= meta-llama/Llama-3.2-3B-Instruct
# Quantization method (examples: Q2_K, Q3_K_M, Q4_K_M, Q5_K_M, Q6_K, Q8_0)
QUANT ?= Q4_K_M
# Optional: Hugging Face repo to upload to (e.g., your-username/Model-quantized)
HF_REPO ?=

# ---- Tooling / environment ----
PYTHON ?= python
CMAKE ?= cmake
GIT ?= git

# Windows vs others: add .exe for binaries on Windows
ifeq ($(OS),Windows_NT)
  EXE := .exe
else
  EXE :=
endif

# ---- llama.cpp settings ----
LLAMA_DIR ?= llama.cpp
LLAMA_BUILD_DIR ?= $(LLAMA_DIR)/build
LLAMA_BIN_DIR ?= $(LLAMA_BUILD_DIR)/bin
CONVERT_SCRIPT := $(LLAMA_DIR)/convert_hf_to_gguf.py

# ---- Output locations ----
FP16_DIR ?= fp16
QUANT_DIR ?= q4
SPLIT_DIR ?= splits
MAX_TENSORS ?= 256

# Derive a readable model name from the path
MODEL_NAME := $(notdir $(MODEL_PATH))

# Output artifact paths
FP16_OUT := $(FP16_DIR)/$(MODEL_NAME)-fp16.gguf
QUANT_OUT := $(QUANT_DIR)/$(MODEL_NAME)-$(QUANT).gguf
SPLIT_PREFIX := $(SPLIT_DIR)/$(MODEL_NAME)-$(QUANT)

.PHONY: help all deps clone-llama build-llama fp16 quant split upload clean distclean

help:
	@echo "Targets:"
	@echo "  deps          Install required Python packages (huggingface_hub, numpy, transformers) and llama.cpp Python deps"
	@echo "  clone-llama   Clone llama.cpp repo if missing"
	@echo "  build-llama   Configure and build llama.cpp with CMake (Release)"
	@echo "  fp16          Convert $${MODEL_PATH} to FP16 GGUF at $(FP16_OUT)"
	@echo "  quant         Quantize FP16 to $(QUANT) at $(QUANT_OUT)"
	@echo "  split         Shard $(QUANT_OUT) into multiple GGUF parts (prefix: $(SPLIT_PREFIX))"
	@echo "  upload        Upload $(QUANT_DIR) to Hugging Face repo $${HF_REPO}"
	@echo "  all           do: clone-llama build-llama deps fp16 quant"
	@echo "  clean         Remove generated FP16/quant/split artifacts"
	@echo "  distclean     Also remove llama.cpp build directory"
	@echo ""
	@echo "Variables you can override:"
	@echo "  MODEL_PATH='path/to/downloaded/hf/model' (current: $(MODEL_PATH))"
	@echo "  QUANT=$(QUANT) (choices: Q2_K, Q3_K_M, Q4_K_M, Q5_K_M, Q6_K, Q8_0)"
	@echo "  HF_REPO='your-user/your-repo' (current: $(HF_REPO))"
	@echo "  LLAMA_DIR=$(LLAMA_DIR)"
	@echo "  LLAMA_BUILD_DIR=$(LLAMA_BUILD_DIR)"
	@echo "  FP16_DIR=$(FP16_DIR)  QUANT_DIR=$(QUANT_DIR)  SPLIT_DIR=$(SPLIT_DIR)  MAX_TENSORS=$(MAX_TENSORS)"

all: clone-llama build-llama deps fp16 quant

# ---- Dependencies ----
deps: clone-llama
	"$(PYTHON)" -m pip install --upgrade pip
	"$(PYTHON)" -m pip install huggingface_hub numpy transformers
	"$(PYTHON)" -m pip install -r "$(LLAMA_DIR)/requirements.txt"

# ---- llama.cpp clone/build ----
$(LLAMA_DIR)/CMakeLists.txt:
	@echo "Cloning llama.cpp into $(LLAMA_DIR)"
	"$(GIT)" clone https://github.com/ggml-org/llama.cpp "$(LLAMA_DIR)"

clone-llama: $(LLAMA_DIR)/CMakeLists.txt

build-llama: clone-llama
	"$(CMAKE)" -E make_directory "$(LLAMA_BUILD_DIR)"
	"$(CMAKE)" -S "$(LLAMA_DIR)" -B "$(LLAMA_BUILD_DIR)" -DCMAKE_BUILD_TYPE=Release $(CMAKE_FLAGS)
	"$(CMAKE)" --build "$(LLAMA_BUILD_DIR)" --config Release -j

# ---- Convert to FP16 GGUF ----
$(FP16_OUT): build-llama deps
	"$(CMAKE)" -E make_directory "$(FP16_DIR)"
	"$(PYTHON)" "$(CONVERT_SCRIPT)" "$(MODEL_PATH)" --outtype f16 --outfile "$(FP16_OUT)"

fp16: $(FP16_OUT)
	@echo "Created: $(FP16_OUT)"

# ---- Quantize to GGUF ----
$(QUANT_OUT): $(FP16_OUT) build-llama
	"$(CMAKE)" -E make_directory "$(QUANT_DIR)"
	"$(LLAMA_BIN_DIR)/llama-quantize$(EXE)" "$(FP16_OUT)" "$(QUANT_OUT)" "$(QUANT)"

quant: $(QUANT_OUT)
	@echo "Created: $(QUANT_OUT)"

# ---- Shard/Split large GGUF ----
split: $(QUANT_OUT)
	"$(CMAKE)" -E make_directory "$(SPLIT_DIR)"
	"$(LLAMA_BIN_DIR)/llama-gguf-split$(EXE)" --split --split-max-tensors $(MAX_TENSORS) "$(QUANT_OUT)" "$(SPLIT_PREFIX)"
	@echo "Split GGUF prefix at: $(SPLIT_PREFIX)"

# ---- Upload quantized directory to Hugging Face ----
upload: quant
	$(if $(strip $(HF_REPO)),,$(error HF_REPO is empty. Set HF_REPO=your-username/your-repo to upload.))
	"huggingface-cli" upload "$(HF_REPO)" "$(QUANT_DIR)"

# ---- Clean ----
clean:
	-"$(CMAKE)" -E remove -f "$(FP16_OUT)"
	-"$(CMAKE)" -E remove -f "$(QUANT_OUT)"
	-"$(CMAKE)" -E remove_directory "$(SPLIT_DIR)"

distclean: clean
	-"$(CMAKE)" -E remove_directory "$(LLAMA_BUILD_DIR)"
