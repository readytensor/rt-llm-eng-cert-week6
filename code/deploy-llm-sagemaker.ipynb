{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ff99bb",
   "metadata": {},
   "source": [
    "# Deploy Fine-tuned LLM to SageMaker\n",
    "\n",
    "This notebook shows how to deploy a LoRA fine-tuned model to AWS SageMaker for inference.\n",
    "\n",
    "## Overview\n",
    "1. Merge LoRA adapters with base model\n",
    "2. Test locally\n",
    "3. Package and upload to S3\n",
    "4. Deploy to SageMaker endpoint\n",
    "5. Run inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e939f6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "337095f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from paths import OUTPUTS_DIR\n",
    "from dotenv import load_dotenv\n",
    "from utils.config_utils import load_config\n",
    "from sagemaker.huggingface import HuggingFaceModel, HuggingFacePredictor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "adapters_dir = os.path.join(OUTPUTS_DIR, \"lora_samsum\", 'lora_adapters')\n",
    "merged_model_dir = os.path.join(OUTPUTS_DIR, \"lora_samsum\", 'merged_model')\n",
    "cfg = load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7fcc0",
   "metadata": {},
   "source": [
    "## Step 1: Merge LoRA Adapters with Base Model\n",
    "\n",
    "SageMaker's default HuggingFace inference toolkit doesn't support loading LoRA adapters on the fly. \n",
    "We need to merge the adapters into the base model first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91392e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week7/data/outputs/lora_samsum/merged_model/tokenizer_config.json',\n",
       " '/Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week7/data/outputs/lora_samsum/merged_model/special_tokens_map.json',\n",
       " '/Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week7/data/outputs/lora_samsum/merged_model/chat_template.jinja',\n",
       " '/Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week7/data/outputs/lora_samsum/merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapters (download from S3 first or use local path)\n",
    "model = PeftModel.from_pretrained(base_model, adapters_dir)\n",
    "\n",
    "# Merge adapters into base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(merged_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapters_dir)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0413db",
   "metadata": {},
   "source": [
    "## Step 2: Test Model Locally\n",
    "\n",
    "Before deploying to SageMaker, test the merged model works correctly on a sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71b4f7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local cache: /Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week7/data/datasets/knkarthick_samsum\n",
      "ðŸ“Š Loaded 14731 train / 200 val / 200 test samples (from full cache).\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import load_and_prepare_dataset, build_messages_for_sample\n",
    "\n",
    "train, val, test = load_and_prepare_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9e3cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = build_messages_for_sample(train[0], cfg[\"task_instruction\"])\n",
    "\n",
    "text_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "409f4f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Amanda baked cookies for Jerry tomorrow.'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "pipe(text_prompt, return_full_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc33ee1",
   "metadata": {},
   "source": [
    "## Step 3: Package and Upload to S3\n",
    "\n",
    "SageMaker needs the model in a tar.gz file stored in S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7e732b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import boto3\n",
    "\n",
    "cfg = load_config()\n",
    "bucket = cfg[\"bucket\"]\n",
    "s3_key = f\"{cfg['output_path']}/merged_model/model.tar.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02efadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tar.gz\n",
    "tar_path = os.path.join(OUTPUTS_DIR, \"lora_samsum\", \"model.tar.gz\")\n",
    "with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "    tar.add(merged_model_dir, arcname=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99480c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded to s3://sagemaker-llm-training-bucket/llama-3-2-1b-instruct/merged_model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(tar_path, bucket, s3_key)\n",
    "\n",
    "print(f\"Uploaded to s3://{bucket}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cc6ee",
   "metadata": {},
   "source": [
    "## Step 4: Deploy to SageMaker Endpoint\n",
    "\n",
    "Create a HuggingFace model and deploy it to a real-time endpoint. This takes ~5-10 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88f4cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "role = os.getenv(\"SAGEMAKER_EXECUTION_ROLE_ARN\")\n",
    "bucket = cfg[\"bucket\"]\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    model_data=f\"s3://{bucket}/{s3_key}\",\n",
    "    role=role,\n",
    "    transformers_version=\"4.51\",\n",
    "    pytorch_version=\"2.6\",\n",
    "    py_version=\"py312\"\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    endpoint_name=\"llama-endpoint\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a76c6",
   "metadata": {},
   "source": [
    "## Step 5: Run Inference\n",
    "\n",
    "Connect to the deployed endpoint and test it with a sample prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e33fdb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Jimmy and Sandy don't want to go to the bar because Trevor is there.\"}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = HuggingFacePredictor(\n",
    "    endpoint_name=\"llama-endpoint\",\n",
    ")\n",
    "\n",
    "predictor.predict({\"inputs\": text_prompt, \"parameters\": {\n",
    "    \"return_full_text\": False, \"do_sample\": False\n",
    "}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c15f49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:11<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "for sample in tqdm(val):\n",
    "    messages = build_messages_for_sample(sample, cfg[\"task_instruction\"])\n",
    "\n",
    "    text_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    out = predictor.predict({\n",
    "        \"inputs\": text_prompt,\n",
    "        \"parameters\": {\"return_full_text\": False, \"do_sample\": False}\n",
    "    })\n",
    "    results.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d94335c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.45472632604924645),\n",
       " 'rouge2': np.float64(0.22621493392056607),\n",
       " 'rougeL': np.float64(0.38015757712025816),\n",
       " 'rougeLsum': np.float64(0.3804240535060933)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = [result[0][\"generated_text\"] for result in results]\n",
    "references = [sample[\"summary\"] for sample in val]\n",
    "\n",
    "rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ad931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
