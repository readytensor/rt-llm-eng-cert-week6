# Default model (can be overridden)
MODEL ?= meta-llama/Llama-3.2-1B-Instruct

# Default tasks
TASKS ?= leaderboard_bbh,leaderboard_gpqa,leaderboard_ifeval,leaderboard_mmlu_pro,leaderboard_musr

# Default device
DEVICE ?= cuda

# Default batch size
BATCH_SIZE ?= auto:4

# HuggingFace token (can be overridden)
HF_TOKEN ?= 

# Setup target - installs dependencies
install:
	git clone https://github.com/EleutherAI/lm-evaluation-harness
	cd lm-evaluation-harness && pip install -e .
	pip install langdetect
	pip install immutabledict

list-tasks:
	lm_eval --tasks list

# Clean setup - removes the cloned repository
clean:
	rm -rf lm-evaluation-harness

# Login to HuggingFace with token argument
login:
ifeq ($(HF_TOKEN),)
	@echo Error: Please provide HF_TOKEN. Usage: make login HF_TOKEN=your_token_here
	@exit 1
else
	huggingface-cli login --token $(HF_TOKEN)
endif

# Run evaluation with default model
eval:
	lm_eval --model hf \
	--model_args pretrained=$(MODEL),parallelize=True \
	--tasks $(TASKS) \
	--device $(DEVICE) \
	--batch_size $(BATCH_SIZE) \
	--trust_remote_code

# Quick evaluation target
quick-eval:
	$(MAKE) eval TASKS=leaderboard_mmlu_pro BATCH_SIZE=auto:2

# Help target
help:
	@echo   install    - Clone repo and install dependencies
	@echo   list-tasks    - List all available evaluation tasks
	@echo   login        - Login to HuggingFace (requires HF_TOKEN=your_token)
	@echo   eval       - Run evaluation with specified model
	@echo   quick-eval - Run quick evaluation with fewer tasks
	@echo   clean      - Remove cloned repository
	@echo   help       - Show this help message
	@echo Variables (can be overridden):
	@echo   MODEL      - Model to evaluate (default: $(MODEL))
	@echo   TASKS      - Tasks to run (default: $(TASKS))
	@echo   DEVICE     - Device to use (default: $(DEVICE))
	@echo   BATCH_SIZE - Batch size (default: $(BATCH_SIZE))
	@echo   HF_TOKEN     - HuggingFace token for login

.PHONY: install login eval quick-eval clean help