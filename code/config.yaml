# ==============================================
# Model Configuration
# ==============================================
base_model: meta-llama/Llama-3.2-1B-Instruct
tokenizer_type: meta-llama/Llama-3.2-1B-Instruct

# ==============================================
# Dataset
# ==============================================
dataset:
  name: knkarthick/samsum
  cache_dir: ../data/datasets
  field_map:
    input: dialogue
    output: summary
  type: completion
  splits:
    train: all
    validation: 200
    test: 200
  seed: 42

task_instruction: >
  You are a helpful assistant who writes concise, factual summaries of conversations.
  Summarize the following conversation into a single sentence.

train_samples: all
val_samples: 200
test_samples: 200
seed: 42

# ==============================================
# Quantization (for QLoRA)
# ==============================================
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: bfloat16

# ==============================================
# LoRA Configuration
# ==============================================
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["q_proj", "v_proj"]

# ==============================================
# Training Configuration
# ==============================================
num_epochs: 1
max_steps: -1
learning_rate: 2e-4
batch_size: 1
gradient_accumulation_steps: 4
sequence_len: 512
lr_scheduler: cosine
warmup_steps: 50
bf16: true
logging_steps: 25
save_steps: 100
save_total_limit: 2
optim: paged_adamw_8bit
report_to: none

# ==============================================
# Output & Logging
# ==============================================
output_dir: ./outputs/lora_samsum
wandb_project: llama3_samsum
wandb_run_name: lora-finetuning-default-hps

# ==============================================
# SageMaker Configuration
# ==============================================
sagemaker_instance_type: ml.p3.2xlarge
sagemaker_instance_count: 1
sagemaker_base_job_name: llama-3-2-1b-instruct-train
bucket: sagemaker-llm-training-bucket
output_path: llama-3-2-1b-instruct

# ==============================================
# Bedrock Configuration
# ==============================================
bedrock_model_inference_profile_id: us.meta.llama3-2-1b-instruct-v1:0
bedrock_job_name: llama-3-2-1b-instruct-train
bedrock_bucket: bedrock-bucket-llm-eng-testing
bedrock_data_dir: bedrock-samsum-dataset
bedrock_batch_outputs_dir: bedrock-samsum-dataset-outputs
